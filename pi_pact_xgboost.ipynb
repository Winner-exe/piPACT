{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pi_pact_sort import bin_categorize, categorize\n",
    "\n",
    "# Config\n",
    "ADDITIONAL_FEATURES = ['HUMIDITY', 'PRESSURE'] # Contains the strings 'HUMIDITY' and/or 'PRESSURE', or none\n",
    "CATEGORIZE_FUNC = bin_categorize # One of the two binning functions in pi_pact_sort\n",
    "\n",
    "# Automatically set objective for XGBClassifier and the file string for pickling\n",
    "if len(ADDITIONAL_FEATURES) > 0:\n",
    "    if len(ADDITIONAL_FEATURES) == 1:\n",
    "        if 'HUMIDITY' in ADDITIONAL_FEATURES:\n",
    "            feature_str = '3varH'\n",
    "        else:\n",
    "            feature_str = '3varP'\n",
    "    else:\n",
    "        feature_str = '4var'\n",
    "else:\n",
    "    feature_str = '2var'\n",
    "\n",
    "if CATEGORIZE_FUNC == categorize:\n",
    "    objective = 'multi:softmax'\n",
    "    num_classes = 3\n",
    "    metric='merror'\n",
    "    scoring='accuracy'\n",
    "    label_str = '3b'\n",
    "else:\n",
    "    objective = 'binary:logistic'\n",
    "    num_classes = None\n",
    "    metric='auc'\n",
    "    scoring='roc_auc'\n",
    "    label_str = 'binary'\n",
    "    \n",
    "file_str = f\"xgboost-models/{feature_str}-{label_str}-xgboost-model.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "DROP_COLUMNS = ['ADDRESS', 'TIMESTAMP', 'UUID', 'MAJOR', 'MINOR', 'TX POWER', 'TEMPERATURE',\n",
    "                'PITCH', 'ROLL', 'YAW', 'SCAN']\n",
    "for feature in ['HUMIDITY', 'PRESSURE']:\n",
    "    if feature not in ADDITIONAL_FEATURES:\n",
    "        DROP_COLUMNS.append(feature)\n",
    "SAMPLE_SIZE = 30000\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "\"\"\"Trains a Gradient Boosted Trees classifier to predict a distance range given RSSI values and other variables.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize DataFrame\n",
    "data: pd.DataFrame = pd.DataFrame(columns=['RSSI', 'DISTANCE'] + ADDITIONAL_FEATURES)\n",
    "data_copy: pd.DataFrame = data.copy()\n",
    "csv_file: Path\n",
    "for csv_file in Path('.').glob('indoor-noObstruct-SenseHat*/*.csv'):\n",
    "    datapart: pd.DataFrame = pd.read_csv(csv_file)\n",
    "    for column in DROP_COLUMNS:\n",
    "        if column in datapart.columns:\n",
    "            datapart = datapart.drop([column], 1)\n",
    "    data_copy = data_copy.append(datapart)\n",
    "\n",
    "# Categorize distance\n",
    "data_copy['DISTANCE'] = data_copy['DISTANCE'].map(CATEGORIZE_FUNC)\n",
    "\n",
    "# Sample data from each distance category\n",
    "for value in data_copy['DISTANCE'].unique():\n",
    "    datapart = data_copy[data_copy.DISTANCE == value]\n",
    "    datapart = datapart.sample(SAMPLE_SIZE, random_state=1)\n",
    "    data = data.append(datapart)\n",
    "\n",
    "# Assign features and labels\n",
    "X: np.array = data.drop(['DISTANCE'], 1).to_numpy()\n",
    "y: np.array = data['DISTANCE'].to_numpy(dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below adapted from Aarshay Jain's \"Complete Guide to Parameter Tuning in XGBoost with codes in Python\", the article can be found at:`https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/` and the full code from the article can be found at: `https://github.com/aarshayj/analytics_vidhya/blob/master/Articles/Parameter_Tuning_XGBoost_with_Example/XGBoost%20models.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, X, y, useTrainCV=True, cv_folds=5, early_stopping_rounds=50) -> int:\n",
    "    \"\"\"\n",
    "    Fits an 'XGBClassifier' instance and provides metrics for hyperparameter tuning.\n",
    "    \n",
    "    Args:\n",
    "        alg (XGBClassifier): the 'XGBClassifier' instance to be fitted.\n",
    "        X (np.array, shape=(n_samples, n_features)): the feature data.\n",
    "        y (np.array, shape=(n_samples, )): the corresponding labels.\n",
    "        useTrainCV (bool): controls whether cross-validation should be used.\n",
    "        cv_folds (int): the number of folds to use for cross-validation.\n",
    "        early_stopping_rounds (int): the maximum number of iterations that may occur without an increase in\n",
    "            cross-validation score before training terminates. This value is passed to the native 'xgb.cv'.\n",
    "            \n",
    "    Returns:\n",
    "        The optimal number of trees given `alg`'s parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(X, label=y, feature_names=['RSSI'] + ADDITIONAL_FEATURES)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'],\n",
    "                          nfold=cv_folds, metrics=metric, early_stopping_rounds=early_stopping_rounds,\n",
    "                          verbose_eval=True)\n",
    "        best_n_estimators = cvresult.shape[0]\n",
    "        alg.set_params(n_estimators=best_n_estimators)\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(X, y, eval_metric=metric)\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(X)\n",
    "       \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(y, dtrain_predictions))\n",
    "    if CATEGORIZE_FUNC == bin_categorize:\n",
    "        dtrain_predprob = alg.predict_proba(X)[:,1]\n",
    "        print(\"AUC Score (Train): %f\" % roc_auc_score(y, dtrain_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "    \n",
    "    if useTrainCV:\n",
    "        return best_n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of Jain's hyperparameter tuning process is to find the optimal number of trees with a fixed learning rate ($\\eta=0.1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(learning_rate =0.1,\n",
    "                     n_estimators=1000,\n",
    "                     max_depth=5,\n",
    "                     min_child_weight=1,\n",
    "                     gamma=0,\n",
    "                     subsample=0.8,\n",
    "                     colsample_bytree=0.8,\n",
    "                     objective=objective,\n",
    "                     num_class=num_classes,\n",
    "                     nthread=4,\n",
    "                     scale_pos_weight=1,\n",
    "                     seed=0)\n",
    "best_n_estimators_1 = modelfit(xgb1, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we tune the integer parameters `max_depth` and `min_child_weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    "     'max_depth': np.arange(1, 8, 1),\n",
    "     'min_child_weight': np.arange(1, 8, 1)\n",
    "}\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate=0.1, n_estimators=best_n_estimators_1,\n",
    "                                                  max_depth=5, min_child_weight=1, gamma=0, subsample=0.8,\n",
    "                                                  colsample_bytree=0.8, objective=objective,\n",
    "                                                  num_class=num_classes, nthread=4, scale_pos_weight=1,\n",
    "                                                  seed=0), \n",
    "                        param_grid = param_test1, scoring=scoring, n_jobs=4, cv=5)\n",
    "gsearch1.fit(X, y)\n",
    "best_max_depth = gsearch1.best_params_['max_depth']\n",
    "best_min_child_weight = gsearch1.best_params_['min_child_weight']\n",
    "print(gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we tune the `gamma` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {\n",
    "     'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, n_estimators=best_n_estimators_1,\n",
    "                                                  max_depth=best_max_depth,\n",
    "                                                  min_child_weight=best_min_child_weight,\n",
    "                                                  gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                  objective=objective, num_class=num_classes,\n",
    "                                                  nthread=4, scale_pos_weight=1, seed=0), \n",
    "                        param_grid = param_test2, scoring=scoring, n_jobs=4, cv=5)\n",
    "gsearch2.fit(X, y)\n",
    "best_gamma = gsearch2.best_params_['gamma']\n",
    "print(gsearch2.cv_results_, gsearch2.best_params_, gsearch2.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, Jain recommends recalibrating the value of `n_estimators` with our new parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2 = XGBClassifier(learning_rate=0.1,\n",
    "                     n_estimators=1000,\n",
    "                     max_depth=best_max_depth,\n",
    "                     min_child_weight=best_min_child_weight,\n",
    "                     gamma=best_gamma,\n",
    "                     subsample=0.8,\n",
    "                     colsample_bytree=0.8,\n",
    "                     objective=objective,\n",
    "                     num_class=num_classes,\n",
    "                     nthread=4,\n",
    "                     scale_pos_weight=1,\n",
    "                     seed=0)\n",
    "best_n_estimators_2 = modelfit(xgb2, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jain's next step is to tune `subsample` and `colsample_bytree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    "     'subsample':[i/100.0 for i in range (75, 85, 1)],\n",
    "     'colsample_bytree':[i/100.0 for i in range(65, 75, 1)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier(learning_rate=0.1, n_estimators=best_n_estimators_2,\n",
    "                                                  max_depth=best_max_depth,\n",
    "                                                  min_child_weight=best_min_child_weight,\n",
    "                                                  gamma=best_gamma, subsample=0.8,\n",
    "                                                  colsample_bytree=0.8,\n",
    "                                                  objective=objective, num_class=num_classes, \n",
    "                                                  nthread=4, scale_pos_weight=1, seed=0), \n",
    "                        param_grid = param_test3, scoring=scoring, n_jobs=4, cv=5)\n",
    "gsearch3.fit(X, y)\n",
    "best_subsample = gsearch3.best_params_['subsample']\n",
    "best_colsample_bytree = gsearch3.best_params_['colsample_bytree']\n",
    "print(gsearch3.cv_results_, gsearch3.best_params_, gsearch3.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we tune `reg_alpha` and `reg_lambda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {\n",
    "     'reg_alpha': [0, 1e-8, 1e-6],\n",
    "     'reg_lamba': [0, ]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = XGBClassifier(learning_rate=0.1, n_estimators=best_n_estimators_2,\n",
    "                                                  max_depth=best_max_depth,\n",
    "                                                  min_child_weight=best_min_child_weight,\n",
    "                                                  gamma=best_gamma, subsample=best_subsample,\n",
    "                                                  colsample_bytree=best_colsample_bytree,\n",
    "                                                  objective=objective, num_class=num_classes,\n",
    "                                                  nthread=4, scale_pos_weight=1, seed=0), \n",
    "                        param_grid=param_test4, scoring=scoring, n_jobs=4, cv=5)\n",
    "gsearch4.fit(X, y)\n",
    "best_alpha = gsearch4.best_params_['reg_alpha']\n",
    "best_lambda = gsearch4.best_params_['reg_lamba']\n",
    "print(gsearch4.cv_results_, gsearch4.best_params_, gsearch4.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finish by reducing $\\eta$ and increasing `n_estimators`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb4 = XGBClassifier(learning_rate=0.01,\n",
    "                     n_estimators=5000,\n",
    "                     max_depth=best_max_depth,\n",
    "                     min_child_weight=best_min_child_weight,\n",
    "                     gamma=best_gamma,\n",
    "                     subsample=best_subsample,\n",
    "                     colsample_bytree=best_colsample_bytree,\n",
    "                     reg_alpha=best_alpha,\n",
    "                     reg_lamba=best_lambda,\n",
    "                     objective=objective,\n",
    "                     num_class=num_classes,\n",
    "                     nthread=4,\n",
    "                     scale_pos_weight=1,\n",
    "                     seed=0)\n",
    "modelfit(xgb4, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print tuned parameters\n",
    "print(f\"Max depth: {best_max_depth}\\n\"\n",
    "      f\"Min child weight: {best_min_child_weight}\\n\"\n",
    "      f\"Gamma: {best_gamma}\\n\"\n",
    "      f\"Colsample by tree: {best_colsample_bytree}\\n\"\n",
    "      f\"Subsample: {best_subsample}\\n\"\n",
    "      f\"Reg alpha: {best_alpha}\\n\"\n",
    "      f\"Reg lamba: {best_lambda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle model for future use\n",
    "with open(file_str, \"wb\") as f:\n",
    "    pickle.dump(xgb4, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Save an roc curve if using binary classification\n",
    "if CATEGORIZE_FUNC == bin_categorize:\n",
    "    probs = xgb4.predict_proba(X)\n",
    "    fpr, tpr, _ = roc_curve(y, probs[:, 1])\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.title('ROC curve')\n",
    "    plt.xlabel('false positive rate')\n",
    "    plt.ylabel('true positive rate')\n",
    "    plt.xlim(0,)\n",
    "    plt.ylim(0,)\n",
    "    plt.savefig(str(Path(f'./xgboost-models/{feature_str}-{label_str}-xgboost-model-roc-curve.png')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
